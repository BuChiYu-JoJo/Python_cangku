import aiohttp
import asyncio
import time
import os
import logging
from datetime import datetime
from collections import deque
from statistics import mean

# ========== é…ç½® ==========
#URL_LIST = [
#    "https://www.google.com",
#    "https://www.bing.com",
#    "https://www.wikipedia.org"
]


URL_LIST = [
    "https://httpbin.org/status/504",  # 5xx é”™è¯¯
#    "http://10.255.255.1",              # è¶…æ—¶
#    "http://nonexistent.openai.invalid"  # å¼‚å¸¸
]



SERVICE_CONFIG = {
    "name": "thordata",
    "endpoint": "https://universalapi.thordata.com/request",
    "headers": {
        "Authorization": "Bearer ff3aaeb7605583f7e51675da4ad2a0de",
        "Content-Type": "application/json"
    },
    "json_payload": lambda url: {
        "url": url,
        "type": "html",
        "js_render": "True"
    }
}

CONCURRENCY = 5
TIMEOUT_SECONDS = 10
MIN_CONTENT_SIZE_KB = 10
THRESHOLD_SUCCESS_RATE = 80.0
THRESHOLD_TIMEOUT_RATE = 10.0

ENABLE_SAVE_RESPONSE = False
ENABLE_CONSOLE_LOG = True
ENABLE_FILE_LOG = True

DINGTALK_WEBHOOK = "https://oapi.dingtalk.com/robot/send?access_token=e05a6891a68b9b3b1cfacf8dcf5852bf647457439261362fac0a1e096951bfa9"
DINGTALK_KEYWORD = "Universal_Scraping_test"

# ========== æ—¥å¿— ==========
class DailyRotatingFileHandler(logging.Handler):
    def __init__(self, log_dir="logs", base_filename="run", encoding="utf-8"):
        super().__init__()
        self.log_dir = log_dir
        self.base_filename = base_filename
        self.encoding = encoding
        os.makedirs(log_dir, exist_ok=True)
        self.current_date = datetime.now().strftime("%Y-%m-%d")
        self.stream = self._open_stream()

    def _get_log_path(self):
        return os.path.join(self.log_dir, f"{self.base_filename}_{self.current_date}.log")

    def _open_stream(self):
        return open(self._get_log_path(), mode="a", encoding=self.encoding)

    def emit(self, record):
        try:
            now = datetime.now().strftime("%Y-%m-%d")
            if now != self.current_date:
                self.current_date = now
                self.stream.close()
                self.stream = self._open_stream()

            msg = self.format(record)
            self.stream.write(msg + "\n")
            self.stream.flush()
        except Exception:
            self.handleError(record)

    def close(self):
        if self.stream:
            try:
                self.stream.close()
            except Exception:
                pass
        super().close()

# è®¾ç½® logger
logger = logging.getLogger("monitor")
logger.setLevel(logging.INFO)

if ENABLE_FILE_LOG:
    handler = DailyRotatingFileHandler(log_dir="logs", base_filename="run")
    handler.setFormatter(logging.Formatter("%(asctime)s - %(message)s"))
    logger.addHandler(handler)

def log_print(msg):
    if ENABLE_CONSOLE_LOG:
        print(msg)
    if ENABLE_FILE_LOG:
        logger.info(msg)

# ========== é’‰é’‰æ¨é€ ==========
async def send_alert(title, content, data_list=None, metric_label="æŒ‡æ ‡å€¼", url=None):
    data_section = ""
    if data_list:
        data_section = f"<br><br>è¿‘3åˆ†é’Ÿ{metric_label}ï¼š<br>{', '.join(str(x) + '%' for x in data_list)}"

    payload = {
        "msgtype": "markdown",
        "markdown": {
            "title": f"{DINGTALK_KEYWORD} | {title}" + (f" | {url}_{datetime.now().strftime('%H:%M:%S')}" if url else ""),
            "text": f"### {DINGTALK_KEYWORD} | {title}<br><br>{content}{data_section}"
        }
    }

    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(DINGTALK_WEBHOOK, json=payload) as resp:
                log_print(f"ğŸ“¢ é’‰é’‰é€šçŸ¥çŠ¶æ€: {resp.status}")
    except Exception as e:
        log_print(f"âŒ é’‰é’‰å‘é€å¼‚å¸¸: {e}")

# ========== å•æ¬¡è¯·æ±‚ ==========
async def fetch(session, url):
    payload = SERVICE_CONFIG["json_payload"](url)
    headers = SERVICE_CONFIG["headers"]
    endpoint = SERVICE_CONFIG["endpoint"]

    start_time = time.time()
    result = {
        "url": url,
        "status": None,
        "elapsed": 0,
        "is_timeout": False,
        "success": False,
        "content_size": 0,
        "timestamp": time.time()
    }

    try:
#        async with session.get(url, timeout=aiohttp.ClientTimeout(total=TIMEOUT_SECONDS)) as resp:  #è°ƒè¯•ç”¨
        async with session.post(endpoint, json=payload, headers=headers,
                                timeout=aiohttp.ClientTimeout(total=TIMEOUT_SECONDS)) as resp:
            text = await resp.text()
            elapsed = time.time() - start_time
            content_size = len(text.encode("utf-8")) / 1024
            status = resp.status

            result.update({
                "status": status,
                "elapsed": elapsed,
                "content_size": content_size,
                "success": (status == 200 and content_size >= MIN_CONTENT_SIZE_KB),
            })

            if status >= 500:
                log_print(f"âš ï¸ 5xx å“åº”: URL={url}, çŠ¶æ€={status}, å¤§å°={content_size:.2f}KB, è€—æ—¶={elapsed:.2f}s")
            else:
                log_print(f"âœ… è¯·æ±‚æˆåŠŸ: URL={url}, çŠ¶æ€={status}, å¤§å°={content_size:.2f}KB, è€—æ—¶={elapsed:.2f}s, æˆåŠŸåˆ¤æ–­={result['success']}")
            return result

    except Exception as e:
        elapsed = time.time() - start_time
        error_type = type(e).__name__
        error_str = str(e) or repr(e)

        result.update({
            "status": 0,
            "elapsed": elapsed,
            "is_timeout": isinstance(e, (asyncio.TimeoutError, aiohttp.ClientTimeout)),
            "success": False
        })

        log_print(f"âŒ è¯·æ±‚å¼‚å¸¸: URL={url}, é”™è¯¯ç±»å‹={error_type}, é”™è¯¯å†…å®¹={error_str}")
        return result


# ========== ä¸»ç›‘æ§å‡½æ•° ==========
async def monitor_loop():
    status_window = {
        url: {
            "success_rate": deque(maxlen=3),
            "timeout_rate": deque(maxlen=3),
            "logger": logger,
        } for url in URL_LIST
    }

    while True:
        current_minute = datetime.now().strftime("%Y-%m-%d %H:%M")
        log_print(f"\n===== â±ï¸ {current_minute} ç›‘æ§å¼€å§‹ =====")

        results = []
        async with aiohttp.ClientSession() as session:
            sem = asyncio.Semaphore(CONCURRENCY)
            tasks = []
            for url in URL_LIST:
                for _ in range(5):
                    async def task(u=url):
                        async with sem:
                            r = await fetch(session, u)
                            results.append(r)
                    tasks.append(task())
            await asyncio.gather(*tasks)

        # æ±‡æ€»å¹¶æ‰“å°æ¯ä¸ª URL çš„ç»Ÿè®¡
        from collections import defaultdict
        grouped = defaultdict(list)
        for r in results:
            grouped[r["url"]].append(r)

        for url, group in grouped.items():
            group.sort(key=lambda x: x.get("timestamp", 0))
            total = len(group)
            success = sum(1 for r in group if r["success"])
            timeout = sum(1 for r in group if r["is_timeout"])
            exceptions = sum(1 for r in group if r["status"] in (500, 502, 504))

            sr = round(success / total * 100, 2)
            tr = round(timeout / total * 100, 2)
            er = round(exceptions / total * 100, 2)

            # æ¯åˆ†é’Ÿç»Ÿè®¡æ‰“å°
            log_print(f"ğŸ“Š {url}ï¼šæˆåŠŸ {success}/{total}ï¼Œè¶…æ—¶ {timeout}ï¼Œå¼‚å¸¸ {exceptions}ï¼ŒæˆåŠŸç‡={sr}%ï¼Œè¶…æ—¶ç‡={tr}%ï¼Œå¼‚å¸¸ç‡={er}%")

            s = status_window[url]
            s["success_rate"].append(sr)
            s["timeout_rate"].append(tr)

            if len(s["success_rate"]) == 3 and all(x < THRESHOLD_SUCCESS_RATE for x in s["success_rate"]):
                await send_alert(
                    f"{url} æˆåŠŸç‡å‘Šè­¦",
                    f"è¿ç»­3åˆ†é’ŸæˆåŠŸç‡ä½äº {THRESHOLD_SUCCESS_RATE}%",
                    list(s["success_rate"]),
                    "æˆåŠŸç‡",
                    url
                )

            if len(s["timeout_rate"]) == 3 and all(x > THRESHOLD_TIMEOUT_RATE for x in s["timeout_rate"]):
                await send_alert(
                    f"{url} è¶…æ—¶ç‡å‘Šè­¦",
                    f"è¿ç»­3åˆ†é’Ÿè¶…æ—¶ç‡é«˜äº {THRESHOLD_TIMEOUT_RATE}%",
                    list(s["timeout_rate"]),
                    "è¶…æ—¶ç‡",
                    url
                )

            # è¿ç»­ 3 æ¬¡ 5xx çŠ¶æ€ç 
            codes = [r["status"] for r in group]
            found_3_5xx = any(
                codes[i] in (500, 502, 504) and
                codes[i + 1] in (500, 502, 504) and
                codes[i + 2] in (500, 502, 504)
                for i in range(len(codes) - 2)
            )
            if found_3_5xx:
                await send_alert(
                    f"{url} è¿ç»­3æ¬¡å¼‚å¸¸è¯·æ±‚",
                    f"{url} å‡ºç°è¿ç»­3æ¬¡ 5xx çŠ¶æ€ç å¼‚å¸¸è¯·æ±‚",
                    url=url
                )

        log_print(f"===== âœ… {current_minute} ç›‘æ§ç»“æŸ =====\n")
        await asyncio.sleep(60)

# ========== å¯åŠ¨ ==========
if __name__ == "__main__":
    asyncio.run(monitor_loop())
