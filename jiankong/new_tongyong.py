import aiohttp
import asyncio
import time
import os
import logging
import csv
from datetime import datetime
from statistics import mean

# ========== é…ç½® ==========
URL_LIST = [
    "https://www.google.com",
    "https://www.bing.com",
    "https://www.wikipedia.org",
]

SERVICE_CONFIG = {
    "name": "thordata",
    "endpoint": "https://universalapi.thordata.com/request",
    "headers": {
        "Authorization": "Bearer ff3aaeb7605583f7e51675da4ad2a0de",
        "Content-Type": "application/json"
    },
    "json_payload": lambda url: {
        "url": url,
        "type": "html",
        "js_render": "True"
    }
}

CONCURRENCY = 10           #å¹¶å‘é…ç½®
TIMEOUT_SECONDS = 30       #æ¯ä¸ªè¯·æ±‚æœ€å¤§ç­‰å¾…æ—¶é—´
MIN_CONTENT_SIZE_KB = 10   #æœ‰æ•ˆè¿”å›å†…å®¹çš„æœ€å°å¤§å°ï¼ˆå°äºè¯¥å€¼è§†ä¸ºå¤±è´¥ï¼‰
RUN_DURATION_SECONDS = 120 #å•è½®æµ‹è¯•æ‰§è¡Œæ—¶é•¿(ç§’)
SCHEDULE_INTERVAL = 300    #å¾ªç¯è¿è¡Œæ—¶é—´é—´éš”(ç§’)
TOTAL_REPEAT = None  # è®¾ç½®ä¸º None è¡¨ç¤ºæ— é™å¾ªç¯ï¼Œç›´åˆ°æ‰‹åŠ¨åœæ­¢ç¨‹åº

ENABLE_SAVE_RESPONSE = False  #æ˜¯å¦ä¿å­˜æˆåŠŸçš„ç½‘é¡µå“åº”å†…å®¹
ENABLE_CONSOLE_LOG = False    #æ˜¯å¦åœ¨æ§åˆ¶å°æ‰“å°æ—¥å¿—ä¿¡æ¯
ENABLE_FILE_LOG = True        #æ˜¯å¦å°†æ—¥å¿—å†™å…¥æœ¬åœ°æ—¥å¿—æ–‡ä»¶logs/run.log

DINGTALK_WEBHOOK = "https://oapi.dingtalk.com/robot/send?access_token=e05a6891a68b9b3b1cfacf8dcf5852bf647457439261362fac0a1e096951bfa9"
DINGTALK_KEYWORD = "test"

# ========== æ—¥å¿—é…ç½® ==========
logger = logging.getLogger("unlocker")
logger.setLevel(logging.INFO)
if ENABLE_FILE_LOG:
    os.makedirs("logs", exist_ok=True)
    fh = logging.FileHandler("logs/run.log", mode="a", encoding="utf-8")
    fh.setFormatter(logging.Formatter("%(asctime)s - %(message)s"))
    logger.addHandler(fh)


def log_print(msg):
    if ENABLE_CONSOLE_LOG:
        print(msg)
    if ENABLE_FILE_LOG:
        logger.info(msg)


# ========== è·å– URL ä¸“å±æ—¥å¿—è®°å½•å™¨ ==========
def get_url_logger(url: str, ts_folder: str):
    domain = url.split("//")[-1].split("/")[0].replace(".", "_")
    log_dir = f"logs/{domain}"
    os.makedirs(log_dir, exist_ok=True)
    logger = logging.getLogger(f"{domain}_{ts_folder}")
    logger.setLevel(logging.INFO)
    if not logger.handlers:
        file_path = os.path.join(log_dir, f"{ts_folder}.log")
        handler = logging.FileHandler(file_path, mode="a", encoding="utf-8")
        handler.setFormatter(logging.Formatter("%(asctime)s - %(message)s"))
        logger.addHandler(handler)
    return logger


# ========== å•æ¬¡è¯·æ±‚ ==========
async def fetch_once(session, semaphore, url, index, ts_folder):
    async with semaphore:
        payload = SERVICE_CONFIG["json_payload"](url)
        headers = SERVICE_CONFIG["headers"]
        endpoint = SERVICE_CONFIG["endpoint"]
        domain_logger = get_url_logger(url, ts_folder)

        result = {
            "url": url,
            "success": False,
            "status": None,
            "error": "",
            "elapsed_ms": 0,
            "content_size_kb": 0,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

        start_time = time.time()
        try:
            async with session.post(endpoint, json=payload, headers=headers,
                                    timeout=aiohttp.ClientTimeout(total=TIMEOUT_SECONDS)) as resp:
                text = await resp.text()
                elapsed = (time.time() - start_time) * 1000
                result.update({
                    "status": resp.status,
                    "elapsed_ms": elapsed,
                    "content_size_kb": len(text.encode("utf-8")) / 1024
                })

                if resp.status != 200:
                    result["error"] = f"HTTP {resp.status}"
                    domain_logger.info(f"[{index}] âŒ HTTP é”™è¯¯, è€—æ—¶={elapsed:.1f}ms")
                    return result

                if result["content_size_kb"] < MIN_CONTENT_SIZE_KB:
                    result["error"] = f"å†…å®¹è¿‡å°: {result['content_size_kb']:.2f}KB"
                    domain_logger.info(f"[{index}] âš  å†…å®¹è¿‡å°, è€—æ—¶={elapsed:.1f}ms")
                    return result

                if ENABLE_SAVE_RESPONSE:
                    folder = f"responses/{ts_folder}"
                    os.makedirs(folder, exist_ok=True)
                    filename = f"{index}_{url.replace('https://', '').replace('/', '_')}.html"
                    with open(os.path.join(folder, filename), "w", encoding="utf-8") as f:
                        f.write(text)

                result["success"] = True
                domain_logger.info(f"[{index}] âœ… æˆåŠŸ, è€—æ—¶={elapsed:.1f}ms, å¤§å°={result['content_size_kb']:.2f}KB")
                return result

        except asyncio.TimeoutError:
            elapsed = (time.time() - start_time) * 1000
            result["error"] = "è¯·æ±‚è¶…æ—¶"
            result["elapsed_ms"] = elapsed
            domain_logger.info(f"[{index}] âŒ è¯·æ±‚è¶…æ—¶, è€—æ—¶={elapsed:.1f}ms")

        except Exception as e:
            elapsed = (time.time() - start_time) * 1000
            result["error"] = f"å¼‚å¸¸: {str(e)}"
            result["elapsed_ms"] = elapsed
            domain_logger.info(f"[{index}] âŒ å¼‚å¸¸: {e}, è€—æ—¶={elapsed:.1f}ms")

        return result


# ========== é’‰é’‰æ¨é€ ==========
async def send_dingtalk(results):
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    lines = [f"### Universal_Scraping_{DINGTALK_KEYWORD} ç»“æœæ±‡æ€» - {now}"]

    for url in URL_LIST:
        url_results = [r for r in results if r["url"] == url]
        total = len(url_results)
        success = [r for r in url_results if r["success"]]
        fail = total - len(success)
        success_rate = round(len(success) / total * 100, 2) if total else 0.0
        avg_time = round(mean(r["elapsed_ms"] for r in success), 2) if success else 0.0
        avg_size = round(mean(r["content_size_kb"] for r in success), 2) if success else 0.0

        lines.append(
            f"\n#### è§£é”ç½‘ç«™ï¼š{url}\n"
            f"- å¹¶å‘æ•°ï¼š{CONCURRENCY}\n"
            f"- è¯·æ±‚æ¬¡æ•°ï¼š{total}\n"
            f"- æˆåŠŸï¼š{len(success)}\n"
            f"- å¤±è´¥ï¼š{fail}\n"
            f"- æˆåŠŸç‡ï¼š{success_rate}%\n"
            f"- å¹³å‡è€—æ—¶ï¼š{avg_time} ms\n"
            f"- å¹³å‡å¤§å°ï¼š{avg_size} KB\n"
        )

    payload = {
        "msgtype": "markdown",
        "markdown": {
            "title": f"{DINGTALK_KEYWORD} æ‰§è¡Œç»“æœ",
            "text": "\n".join(lines)
        }
    }

    async with aiohttp.ClientSession() as session:
        async with session.post(DINGTALK_WEBHOOK, json=payload) as resp:
            if resp.status == 200:
                log_print("ğŸ“¢ é’‰é’‰é€šçŸ¥å·²å‘é€")
            else:
                log_print(f"âŒ é’‰é’‰é€šçŸ¥å¤±è´¥: {resp.status}")

    log_print("ğŸ“Š é’‰é’‰æ¨é€å†…å®¹ï¼š\n" + "\n".join(lines))


# ========== å•è½®æµ‹è¯• ==========
async def run_once():
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    semaphore = asyncio.Semaphore(CONCURRENCY)
    results = []
    index = 0
    per_url_duration = RUN_DURATION_SECONDS / len(URL_LIST)

    async with aiohttp.ClientSession() as session:
        for url in URL_LIST:
            log_print(f"â–¶ å¼€å§‹ URL: {url}ï¼ˆåˆ†é…æ—¶é•¿çº¦ {per_url_duration:.1f}sï¼‰")
            end_time = time.time() + per_url_duration

            while time.time() < end_time:
                tasks = []
                for _ in range(CONCURRENCY):
                    index += 1
                    tasks.append(fetch_once(session, semaphore, url, index, timestamp))
                batch_results = await asyncio.gather(*tasks)
                results.extend(batch_results)

    os.makedirs("reports", exist_ok=True)
    csv_file = f"reports/result_{timestamp}.csv"
    with open(csv_file, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=results[0].keys())
        writer.writeheader()
        writer.writerows(results)

    log_print(f"ğŸ“ ç»“æœå·²ä¿å­˜ï¼š{csv_file}")
    await send_dingtalk(results)


# ========== å¾ªç¯è°ƒåº¦ ==========
async def scheduled_loop():
    count = 0
    while TOTAL_REPEAT is None or count < TOTAL_REPEAT:
        log_print(f"\nğŸ” å¼€å§‹ç¬¬ {count + 1} è½®æµ‹è¯•ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        await run_once()
        count += 1
        log_print(f"âœ… ç¬¬ {count} è½®å®Œæˆï¼Œç­‰å¾… {SCHEDULE_INTERVAL} ç§’...\n")
        await asyncio.sleep(SCHEDULE_INTERVAL)


# ========== å¯åŠ¨å…¥å£ ==========
if __name__ == "__main__":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    asyncio.run(scheduled_loop())
