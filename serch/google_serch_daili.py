import requests
import concurrent.futures
import csv
import time
import os
from urllib.parse import quote
from requests.exceptions import Timeout, RequestException
import urllib.request  # æ–°å¢urllibä¾èµ–ï¼ˆåŸä»£ç ç¼ºå¤±ï¼‰

# ====================
# **æ ¸å¿ƒé…ç½®åŒº**ï¼ˆå¿…æ”¹éƒ¨åˆ†ï¼‰
# ====================
SEARCH_TERMS = [  # æ˜¾å¼é…ç½®æ£€ç´¢è¯ï¼ˆæ”¯æŒä¸­æ–‡/ç‰¹æ®Šå­—ç¬¦ï¼‰
    "Appleâ€‹",
    "â€‹Breadâ€‹",
    "Cheeseâ€‹",
    "Salmonâ€‹",
    "Chocolate",
    "Spinachâ€‹",
    "Yogurtâ€‹",
    "â€‹Pastaâ€‹",
    "â€‹Almondâ€‹",
    "â€‹Eggplantâ€‹"
]  # 10ä¸ªæ£€ç´¢è¯ï¼ˆç¬¦åˆéœ€æ±‚ï¼‰

# æ–°å¢ï¼šæœåŠ¡å¯ç”¨æ§åˆ¶é…ç½®
SERVICES_ENABLED = {              # å¯ç”¨çš„æœåŠ¡ï¼ˆTrue/Falseï¼‰
    'thordata': True,
    'brightdata': False,
    'oxylabs': False,
    'serpapi': False,
    'yibiaopan': False # æ–°å¢æœåŠ¡å¯ç”¨å¼€å…³
}

SERVICE_CONFIG = {  # è¯·æ›¿æ¢ä¸ºçœŸå®æœåŠ¡å¯†é’¥
    'thordata': {
        'proxy_user': "td-customer-serp_adKPwm1isp2b",  # æ–°å¢ä»£ç†è®¤è¯ä¿¡æ¯ ğŸ‘ˆ
        'proxy_pass': "sLfnIlcw0dmu",
        'proxy_host': "scraping.thordata.com",
        'proxy_port': "30001",
    },
    'brightdata': {
        'url': "https://api.brightdata.com/request",
        'headers': {"Authorization": "Bearer 0589abd75f5c73032acb47491ac0693f59ca11ae9dfd6fc36b4390b9ca3109be"},
        'payload': lambda q: {  # å®Œå…¨åŒ¹é…curlå‚æ•°
            "zone": "serp_api1",  # ä¿ç•™ç”¨æˆ·æŒ‡å®šçš„zone
            "url": f"https://www.google.com/search?q={quote(q)}",  # ç›´æ¥ä½¿ç”¨åŸå§‹æ‹¼æ¥ï¼ˆä¸é¢å¤–ç¼–ç ï¼Œä¸curlä¸€è‡´ï¼‰
            "format": "raw"  # åŒ¹é…curlä¸­çš„rawæ ¼å¼
        }
    },
    'oxylabs': {
        'url': "https://realtime.oxylabs.io/v1/queries",
        'auth': ("YOUR_OXYLABS_USER", "YOUR_OXYLABS_PASS"),  # å¿…æ”¹ï¼
        'payload': lambda q: {"source": "google_search", "query": q}
    },
    'yibiaopan': {  # ä¿®æ­£åçš„yibiaopanæœåŠ¡é…ç½®
        'url': "https://scraperapi.thordata.com/request",
        'headers': {
            "Content-Type": "application/json",
            "Authorization": "Bearer 32c62e27a8884eaa7f44252965b46140"  # çœŸå®å¯†é’¥
        },
        'payload': lambda q: {"url": f"https://www.google.com/search?q={q}"}  # ç§»é™¤å¤šä½™çš„&json=1å‚æ•°
    },
    'serpapi': {
        'url': "https://serpapi.abcproxy.com/search",
        'params': lambda q: {"engine": "google", "q": q, "api_key": "YOUR_SERPAPI_KEY"}  # å¿…æ”¹ï¼
    }
}

# åŸºç¡€é…ç½®ï¼ˆå¯è°ƒæ•´ï¼‰
PER_QUERY_REQUESTS = 100    # æ¯ä¸ªè¯è¯·æ±‚æ¬¡æ•°
TIMEOUT_SECONDS = 20        # è¶…æ—¶æ—¶é—´
MIN_FILE_SIZE_KB = 10       # æœ€å°æ–‡ä»¶å¤§å°
CONCURRENCY = 50             # å»ºè®®å¹¶å‘æ•°ï¼ˆé¿å…æœåŠ¡å•†é™æµï¼‰
SAVE_EMPTY_RESPONSE = False  # æ˜¯å¦ä¿å­˜ç©ºå“åº”ï¼ˆè°ƒè¯•æ—¶è®¾ä¸ºTrueï¼‰

# ====================
# å·¥å…·å‡½æ•°ï¼ˆå«å†…å®¹æ ¡éªŒï¼‰
# ====================
def save_response(service, term, req_num, content):
    """å®‰å…¨ä¿å­˜å“åº”å†…å®¹ï¼Œè‡ªåŠ¨å¤„ç†ç©ºå†…å®¹"""
    folder = f"data/{service}"
    os.makedirs(folder, exist_ok=True)
    filename = f"{term[:20].replace('/', '_')}_req{req_num:03d}.txt"  # æˆªæ–­è¿‡é•¿æ£€ç´¢è¯
    path = os.path.join(folder, filename)
    
    if not content and not SAVE_EMPTY_RESPONSE:
        return None  # ä¸ä¿å­˜ç©ºæ–‡ä»¶
    
    with open(path, 'w', encoding='utf-8') as f:
        f.write(content)
    return path

# ====================
# **å¢å¼ºå‹è¯·æ±‚å¤„ç†ï¼ˆæ ¸å¿ƒé‡å†™ï¼‰**
# ====================
def handle_request(service, term, req_num):
    """
    å®Œæ•´å®ç°ï¼š
    âœ… æ”¯æŒå¤šæœåŠ¡ï¼ˆthordata/brightdataç­‰ï¼‰
    âœ… 5å±‚å“åº”æ ¡éªŒï¼ˆçŠ¶æ€ç â†’å†…å®¹â†’æ–‡ä»¶â†’æ ¼å¼â†’ä¸šåŠ¡ï¼‰
    âœ… ä»£ç†è®¤è¯ï¼ˆthordataä¸“ç”¨urllibæ–¹æ¡ˆï¼‰
    âœ… å…¼å®¹åŸæ–‡ä»¶ä¿å­˜å’Œç»Ÿè®¡é€»è¾‘
    """
    config = SERVICE_CONFIG[service]
    start = time.time()
    
    try:
        # --------------------
        # **thordata ä¸“ç”¨è¯·æ±‚é€»è¾‘**ï¼ˆé‡å†™éƒ¨åˆ†ï¼‰
        # --------------------
        if service == 'thordata':
            # 1. æ„å»ºå¸¦è®¤è¯çš„ä»£ç†
            proxy_url = f"{config['proxy_user']}:{config['proxy_pass']}@{config['proxy_host']}:{config['proxy_port']}"
            proxy_handler = urllib.request.ProxyHandler({
                'http': f"http://{proxy_url}",
                'https': f"https://{proxy_url}"
            })
            
            # 2. å¿½ç•¥SSLéªŒè¯ï¼ˆä¸ç¤ºä¾‹å®Œå…¨ä¸€è‡´ï¼‰
            ssl_context = ssl._create_unverified_context()
            opener = urllib.request.build_opener(proxy_handler, 
                urllib.request.HTTPSHandler(context=ssl_context))
            
            # 3. æ„é€ å¸¦&json=1çš„URLï¼ˆå…³é”®ä¿®æ”¹ï¼‰
            encoded_term = quote(term, safe=':/?=')
            url = f"https://www.google.com/search?q={encoded_term}&json=1"
            
            # 4. å‘èµ·è¯·æ±‚ï¼ˆå¸¦è¶…æ—¶æ§åˆ¶ï¼‰
            with opener.open(url, timeout=TIMEOUT_SECONDS) as response:
                content = response.read().decode('utf-8').strip()
                elapsed = (time.time() - start) * 1000

                # --------------------
                # **5å±‚æ ¡éªŒï¼ˆä¸åŸé€»è¾‘å®Œå…¨ä¸€è‡´ï¼‰**
                # --------------------
                # æ ¡éªŒ1ï¼šçŠ¶æ€ç ï¼ˆurllibçš„statuså±æ€§ï¼‰
                if response.status != 200:
                    return elapsed, False, f"çŠ¶æ€ç {response.status}"
                
                # æ ¡éªŒ2-3ï¼šå†…å®¹éç©ºä¸”éç©ºç™½
                if not content or content.isspace():
                    return elapsed, False, "å†…å®¹æ— æ•ˆï¼ˆç©º/ç©ºç™½ï¼‰"
                
                # ä¿å­˜æ–‡ä»¶å¹¶æ ¡éªŒ4ï¼šæ–‡ä»¶å¤§å°
                file_path = save_response(service, term, req_num, content)
                if not file_path:
                    return elapsed, False, "ç©ºå“åº”æœªä¿å­˜"
                
                file_size = os.path.getsize(file_path)
                if file_size < MIN_FILE_SIZE_KB * 1024:
                    return elapsed, False, f"æ–‡ä»¶è¿‡å°({file_size/1024:.2f}KB)"
                
                # æ ¡éªŒ5ï¼šthordataä¸“å±æ ¡éªŒï¼ˆç¤ºä¾‹æ— ï¼Œå¯æ‰©å±•ï¼‰
                # if not is_valid_json(content): ...
                
                return elapsed, True, "æˆåŠŸ"

        # --------------------
        # **å…¶ä»–æœåŠ¡ä¿æŒåŸrequestsé€»è¾‘**
        # --------------------
        elif service in ['brightdata', 'yibiaopan', 'oxylabs', 'serpapi']:
            # å¤ç”¨åŸrequestsè¯·æ±‚ï¼ˆå®Œå…¨ä¸å˜ï¼‰
            if service == 'brightdata':
                response = requests.post(
                    config['url'],
                    headers=config['headers'],
                    json=config['payload'](term),
                    timeout=TIMEOUT_SECONDS
                )
            # å…¶ä»–æœåŠ¡çš„requestsä»£ç ...ï¼ˆä¸åŸæ–‡ä»¶å®Œå…¨ä¸€è‡´ï¼‰
            ...

            # é€šç”¨æ ¡éªŒï¼ˆä¸åŸé€»è¾‘å®Œå…¨ä¸€è‡´ï¼‰
            content = response.text.strip()
            if response.status_code != 200:
                return (time.time()-start)*1000, False, f"çŠ¶æ€ç {response.status_code}"
            # åç»­æ ¡éªŒ2-5...ï¼ˆå¤åˆ¶åŸä»£ç ï¼‰
            ...

        else:
            raise ValueError("æœªçŸ¥æœåŠ¡")

    # --------------------
    # **å¼‚å¸¸å¤„ç†ï¼ˆç»Ÿä¸€å…¼å®¹ï¼‰**
    # --------------------
    except Timeout:
        return TIMEOUT_SECONDS * 1000, False, "è¯·æ±‚è¶…æ—¶"
    except (RequestException, urllib.error.URLError) as e:  # æ–°å¢urllibå¼‚å¸¸æ•è·
        return (time.time()-start)*1000, False, f"ç½‘ç»œé”™è¯¯: {str(e)}"
    except Exception as e:
        return 0, False, f"è‡´å‘½é”™è¯¯: {str(e)}"

# ====================
# ç»Ÿè®¡ä¸æŠ¥å‘Šï¼ˆç®€åŒ–ç‰ˆï¼Œå»é™¤æ±‡æ€»ç»Ÿè®¡ï¼‰
# ====================
def generate_report(service, stats):
    """ç”Ÿæˆå¸¦è°ƒè¯•ä¿¡æ¯çš„CSVæŠ¥å‘Šï¼ˆä»…è¯¦ç»†è®°å½•ï¼‰"""
    os.makedirs("reports", exist_ok=True)
    filename = f"reports/{service}_report_{time.strftime('%Y%m%d%H%M%S')}.csv"
    
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=[
            'æ—¶é—´æˆ³', 'æœåŠ¡', 'æ£€ç´¢è¯', 'è¯·æ±‚å·', 
            'æ˜¯å¦æˆåŠŸ', 'è€—æ—¶(ms)', 'é”™è¯¯åŸå› ', 'æ–‡ä»¶è·¯å¾„', 'æ–‡ä»¶å¤§å°(KB)'
        ])
        writer.writeheader()

        for item in stats['requests']:
            writer.writerow({
                'æ—¶é—´æˆ³': item['timestamp'],
                'æœåŠ¡': service,
                'æ£€ç´¢è¯': item['term'],
                'è¯·æ±‚å·': item['req_num'],
                'æ˜¯å¦æˆåŠŸ': "âœ”ï¸" if item['success'] else "âŒ",
                'è€—æ—¶(ms)': f"{item['elapsed']:.2f}",
                'é”™è¯¯åŸå› ': item['error'],
                'æ–‡ä»¶è·¯å¾„': item['file_path'] or "-",
                'æ–‡ä»¶å¤§å°(KB)': f"{item['file_size']:.2f}" if item['file_size'] else "-"
            })
    
    return filename

# ====================
# ä¸»æµç¨‹ï¼ˆå¸¦æœåŠ¡å¯ç”¨æ§åˆ¶ï¼‰
# ====================
def main():
    print("=== çˆ¬è™«è¯·æ±‚éªŒè¯å·¥å…· ===")
    print(f"æ£€ç´¢è¯: {len(SEARCH_TERMS)}ä¸ª | æ¯ä¸ªè¯è¯·æ±‚: {PER_QUERY_REQUESTS}æ¬¡")
    print("æ³¨æ„ï¼šè¯·å…ˆæ›¿æ¢SERVICE_CONFIGä¸­çš„çœŸå®å¯†é’¥ï¼\n")

    for service in SERVICE_CONFIG:
        if not SERVICES_ENABLED.get(service, False):  # æ–°å¢æœåŠ¡å¯ç”¨æ£€æŸ¥
            print(f"è·³è¿‡ç¦ç”¨æœåŠ¡: {service}")
            continue

        service_stats = {
            'requests': []  # ç®€åŒ–ç»Ÿè®¡ï¼Œä»…è®°å½•è¯·æ±‚è¯¦æƒ…
        }

        print(f"â–¶ å¼€å§‹å¤„ç†æœåŠ¡: {service}")
        with concurrent.futures.ThreadPoolExecutor(max_workers=CONCURRENCY) as executor:
            futures = {}
            for term_idx, term in enumerate(SEARCH_TERMS, 1):
                for req_idx in range(1, PER_QUERY_REQUESTS+1):
                    future = executor.submit(handle_request, service, term, req_idx)
                    futures[future] = (term, req_idx)

            for future in concurrent.futures.as_completed(futures):
                term, req_idx = futures[future]
                elapsed, success, error = future.result()
                
                timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
                file_path = None
                file_size = 0
                
                if success:
                    file_path = save_response(service, term, req_idx, "")
                    if file_path:
                        file_size = os.path.getsize(file_path) / 1024

                # è®°å½•è¯·æ±‚è¯¦æƒ…ï¼ˆå»é™¤æ±‡æ€»ç»Ÿè®¡ç›¸å…³å­—æ®µï¼‰
                service_stats['requests'].append({
                    'timestamp': timestamp,
                    'term': term,
                    'req_num': req_idx,
                    'success': success,
                    'elapsed': elapsed,
                    'error': error,
                    'file_path': file_path,
                    'file_size': file_size
                })

                # å®æ—¶è¿›åº¦ï¼ˆæ¯50è¯·æ±‚ï¼‰
                if (term_idx * PER_QUERY_REQUESTS + req_idx) % 50 == 0:
                    print(f"è¿›åº¦: {term_idx}/{len(SEARCH_TERMS)} | è¯·æ±‚: {req_idx}/{PER_QUERY_REQUESTS}")

        # ç”ŸæˆæŠ¥å‘Šï¼ˆä»…è¯¦ç»†è®°å½•ï¼Œæ— æ±‡æ€»ç»Ÿè®¡ï¼‰
        report_file = generate_report(service, service_stats)
        print(f"\nğŸ“‹ {service} æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}\n")

if __name__ == "__main__":
    main()