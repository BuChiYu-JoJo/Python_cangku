import aiohttp
import asyncio
import csv
import os
import time
import logging
import random
import traceback
import uuid
from urllib.parse import quote, urlencode
from datetime import datetime

# ========= é…ç½® =========
SEARCH_ENGINES = {
    "yandex": {
        "enabled": True,
        "domain": ["yandex.com", "yandex.ru", "yandex.uz", "yandex.kz", "yandex.com.tr"],
        "param": "text",
        "extra_params": ["lang", "lr", "rstr", "p", "within"]
    },
    "duckduckgo": {
        "enabled": False,
        "domain": "duckduckgo.com",
        "param": "q",
        "extra_params": ["kl", "start", "df", "kp"]
    },
    "google": {
        "enabled": False,
        "domain": ["www.google.com", "www.google.ad", "www.google.ca", "www.google.co.jp", "www.google.com.vn"],
        "param": "q",
        "extra_params": ["gl", "hl", "cr", "lr", "location", "uule", "tbm", "start", "num", "safe"]
    },
    "bing": {
        "enabled": False,
        "domain": "www.bing.com",
        "param": "q",
        "extra_params": ["cc", "mkt", "location", "lat", "lon", "first", "count", "adlt"]
    },
}

SEARCH_TERMS = [
    "Apple", "Bread", "Cheese", "Salmon", "Chocolate",
    "Spinach", "Yogurt", "Pasta", "Almond", "Eggplant"
]

AUTH_HEADER = {
    "Authorization": "Bearer 5d7caa7f1e33019f9b1851e179415bc9",
    "Content-Type": "application/json"
}

CONCURRENCY = 1
TIMEOUT_SECONDS = 60
MIN_CONTENT_SIZE_KB = 0
ENABLE_SAVE_RESPONSE = True
REQUEST_COUNT_PER_ENGINE = 1000

# ========= æ—¥å¿—é…ç½® =========
def get_logger(engine_name):
    os.makedirs("logs", exist_ok=True)
    logger = logging.getLogger(engine_name)
    if not logger.handlers:
        handler = logging.FileHandler(f"logs/{engine_name}.log", encoding="utf-8")
        formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
    return logger

# ========= æ„å»ºæŸ¥è¯¢å‚æ•° =========
def random_param_value(param, engine=None):
    if param == "hl":
        return random.choice(["en", "fr", "ru", "ja", "cn"])
    elif param == "lang":
        return random.choice(["en", "ru", "fr", "de", "ua"])
    elif param == "lr":
        if engine == "google":
            return random.choice(["lang_en", "lang_ja", "lang_ca", "lang_ru", "lang_zh-CN"])
        elif engine == "yandex":
            return random.choice(["United+States", "Japan", "Germany", "France", "China"])
        else:
            return None
    elif param == "mkt":
        return random.choice(["en-us", "ja-jp", "fr-ca", "ru-ru", "zh-cn"])
    elif param == "location":
        # æ‹†åˆ†å‡º location å’Œ uule ä¸¤ä¸ªå‚æ•°
        loc_uule_pairs = [
            ("India", "w+CAIQICIFSW5kaWE"),
            ("United States", "w+CAIQICINVW5pdGVkK1N0YXRlcw"),
            ("Japan", "w+CAIQICIFSmFwYW4"),
            ("Brazil", "w+CAIQICIGQnJhemls"),
            ("France", "w+CAIQICIGRnJhbmNl")
        ]
        loc, uule = random.choice(loc_uule_pairs)
        return {"location": loc, "uule": uule}
    elif param == "cr":
        return random.choice(["countryAF", "countryCA", "countryCN", "countryJP", "countryUS"])
    elif param == "gl":
        return random.choice(["us", "uk", "ru", "jp", "cn"])
    elif param == "rstr":
        return random.choice(["true", None])
    elif param == "within":
        return random.choice(["0", "77", "1", "2"])
    elif param == "kl":
        return random.choice(["India", "United+States", "Japan", "France", "China"])
    elif param == "df":
        return random.choice([None, "d", "w", "m", "y"])
    elif param == "kp":
        return random.choice(["1", "-1", "-2"])
    elif param == "tbm":
        return random.choice([None, "isch", "shop", "nws", "vid"])
    elif param == "start":
        return str(random.randint(0, 20))
    elif param == "num":
        return str(random.choice([5, 10, 15]))
    elif param == "first":
        return str(random.randint(0, 20))
    elif param == "count":
        return str(random.randint(0, 50))
    elif param == "adlt":
        return random.choice(["strict", "off", None])
    elif param == "safe":
        return random.choice(["active", "off",None])
    elif param in {"lat", "lon"}:
        return str(round(random.uniform(-90, 90), 4))
    else:
        return str(random.randint(0, 10))

# ========= æ„å»ºè¯·æ±‚ =========
def build_payload(engine, term):
    config = SEARCH_ENGINES[engine]
    domain = random.choice(config["domain"]) if isinstance(config["domain"], list) else config["domain"]
    base_url = f"https://{domain}/search"

    query = {
        config['param']: term,
        "json": "1"
    }

    used_keys = set()  # è·Ÿè¸ªå·²æ·»åŠ çš„å‚æ•°
    for param in config.get("extra_params", []):
        if param in used_keys:
            continue
        value = random_param_value(param, engine)
        if value is None:
            continue
        if isinstance(value, dict):
            for k, v in value.items():
                if k not in used_keys:
                    query[k] = v
                    used_keys.add(k)
        else:
            if param not in used_keys:
                query[param] = value
                used_keys.add(param)

    query_str = urlencode(query)
    full_url = f"{base_url}?{query_str}"
    return {
        "url": full_url,
        "full_url": full_url
    }

# ========= è¯·æ±‚å‡½æ•° =========
async def fetch(session, engine, term):
    start = time.time()
    payload = build_payload(engine, term)
    full_url = payload["full_url"]
    logger = get_logger(engine)

    base_result = {
        "engine": engine,
        "term": term,
        "status": 0,
        "elapsed": 0,
        "success": False,
        "content_size_kb": 0,
        "full_url": full_url,
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "exception": "",
        "code": None,
        "response_saved": False
    }

    try:
        async with session.post(
            url="https://scraperapi.thordata.com/request",
            headers=AUTH_HEADER,
            json={"url": full_url},
            timeout=aiohttp.ClientTimeout(total=TIMEOUT_SECONDS)
        ) as response:
            content = await response.text()
            elapsed = time.time() - start
            content_size = len(content.encode("utf-8")) / 1024
            success = False
            code = None

            try:
                json_data = await response.json()
                code = json_data.get("code")
                data = json_data.get("data", {})
                success = (
                    response.status == 200 and
                    code == 200 and
                    isinstance(data, dict) and
                    "task_id" in data
                )
                if not success:
                    if isinstance(data, dict):
                        base_result["exception"] = (
                                data.get("message") or
                                data.get("error") or
                                data.get("msg") or
                                str(data)
                        )
                    elif isinstance(data, str):
                        base_result["exception"] = data
                    else:
                        base_result["exception"] = str(data)

            except Exception as parse_err:
                base_result["exception"] = f"JSONè§£æå¤±è´¥: {type(parse_err).__name__}: {str(parse_err)}"

            # ä¿å­˜å“åº”å†…å®¹ï¼ˆæ— è®ºæˆåŠŸä¸å¦ï¼‰
            if ENABLE_SAVE_RESPONSE:
                save_dir = f"responses/{engine}"
                os.makedirs(save_dir, exist_ok=True)
                status_str = "success" if success else "fail"
                safe_term = quote(term, safe='')
                filename = f"{save_dir}/{safe_term}_{status_str}_{int(time.time() * 1000)}_{uuid.uuid4().hex[:6]}.json"

                try:
                    with open(filename, "w", encoding="utf-8") as f:
                        f.write(content if content else "ï¼ˆå“åº”ä¸ºç©ºï¼‰")
                    base_result["response_saved"] = True
                except Exception as file_err:
                    logger.error(f"[{engine}] æ–‡ä»¶ä¿å­˜å¤±è´¥: {filename}\né”™è¯¯: {file_err}")

            base_result.update({
                "status": response.status,
                "elapsed": round(elapsed, 2),
                "success": success,
                "content_size_kb": round(content_size, 2),
                "code": code
            })

            if success:
                logger.info(
                    f"{term} çŠ¶æ€: {response.status} | æˆåŠŸ | è€—æ—¶: {elapsed:.2f}s | å¤§å°: {content_size:.2f}KB | å·²ä¿å­˜: {base_result['response_saved']}\nURL: {full_url}"
                )
            else:
                logger.error(
                    f"{term} çŠ¶æ€: {response.status} | ä¸šåŠ¡å¤±è´¥ | è€—æ—¶: {elapsed:.2f}s | å¤§å°: {content_size:.2f}KB"
                    + f" | é”™è¯¯: {base_result['exception']}" if base_result["exception"] else ""
                    + f" | å·²ä¿å­˜: {base_result['response_saved']}\nURL: {full_url}"
                )

            return base_result

    except Exception as e:
        elapsed = time.time() - start
        tb_str = traceback.format_exc()
        error_type = type(e).__name__
        error_message = str(e)
        exception_text = f"{error_type}: {error_message}" if error_message else error_type
        base_result.update({
            "elapsed": round(elapsed, 2),
            "exception": exception_text
        })

        # å¼‚å¸¸æƒ…å†µä¸‹å°è¯•ä¿å­˜é”™è¯¯è¯¦æƒ…
        if ENABLE_SAVE_RESPONSE:
            save_dir = f"responses/{engine}"
            os.makedirs(save_dir, exist_ok=True)
            safe_term = quote(term, safe='')
            filename = f"{save_dir}/{safe_term}_exception_{int(time.time() * 1000)}_{uuid.uuid4().hex[:6]}.txt"
            try:
                with open(filename, "w", encoding="utf-8") as f:
                    f.write(f"è¯·æ±‚å¼‚å¸¸ï¼š{exception_text}\n")
                    f.write(f"URL: {full_url}\n\nå †æ ˆä¿¡æ¯:\n{tb_str}")
                base_result["response_saved"] = True
            except Exception as file_err:
                logger.error(f"[{engine}] å¼‚å¸¸æ–‡ä»¶ä¿å­˜å¤±è´¥: {filename}\né”™è¯¯: {file_err}")

        logger.error(
            f"[{engine}] è¯·æ±‚å¤±è´¥: {term} | é”™è¯¯: {exception_text} | å·²ä¿å­˜: {base_result['response_saved']}\nURL: {full_url}\nå †æ ˆ:\n{tb_str}"
        )
        return base_result


# ========= æ¯ä¸ªå¼•æ“è½®è¯¢è¯·æ±‚ =========
async def run_for_engine(engine, term_list):
    sem = asyncio.Semaphore(CONCURRENCY)
    results = []
    engine_logger = get_logger(engine)

    async with aiohttp.ClientSession() as session:
        async def task(term):
            async with sem:
                result = await fetch(session, engine, term)
                results.append(result)

        tasks = [asyncio.create_task(task(term)) for term in term_list]
        await asyncio.gather(*tasks)

    return results

# ========= ä¸»è°ƒåº¦å™¨ =========
async def run_all():
    for engine in SEARCH_ENGINES:
        config = SEARCH_ENGINES[engine]
        engine_logger = get_logger(engine)

        if not config.get("enabled", False):
            engine_logger.info(f"â© å¼•æ“ {engine} è¢«ç¦ç”¨ï¼Œè·³è¿‡")
            continue

        engine_logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œå¼•æ“ï¼š{engine}")
        terms = random.choices(SEARCH_TERMS, k=REQUEST_COUNT_PER_ENGINE)

        start_time = time.time()
        results = await run_for_engine(engine, terms)
        end_time = time.time()
        elapsed_total = round(end_time - start_time, 2)

        save_single_engine_to_csv(engine, results, elapsed_total)

# ========= ä¿å­˜ CSV =========
def save_single_engine_to_csv(engine, data, total_elapsed):
    os.makedirs("csv", exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    fieldnames = [
        "engine", "term", "status", "code", "success", "content_size_kb",
        "elapsed", "full_url", "exception"
    ]
    header_mapping = {
        "engine": "å¼•æ“",
        "term": "å…³é”®è¯",
        "status": "çŠ¶æ€ç ",
        "code": "ä¸šåŠ¡çŠ¶æ€ç ",
        "success": "æ˜¯å¦æˆåŠŸ",
        "content_size_kb": "å“åº”å†…å®¹å¤§å°(KB)",
        "elapsed": "å“åº”æ—¶é—´(s)",
        "full_url": "å®Œæ•´è¯·æ±‚URL",
        "exception": "é”™è¯¯å¤‡æ³¨"
    }

    if not data:
        print(f"âš ï¸ å¼•æ“ {engine} æ— æ•°æ®å¯ä¿å­˜")
        return

    # è®¡ç®—ç»Ÿè®¡æ•°æ®
    total = len(data)
    success_data = [r for r in data if r.get("success")]
    success_count = len(success_data)
    success_rate = f"{(success_count / total * 100):.2f}%" if total else "0.00%"
    avg_size = round(sum(r.get("content_size_kb", 0) for r in success_data) / success_count, 2) if success_count else 0
    avg_elapsed = round(sum(r.get("elapsed", 0) for r in success_data) / success_count, 2) if success_count else 0

    # å‡†å¤‡å¹³å‡è¡Œï¼ˆå­—æ®µå€¼ç”¨ç©ºå­—ç¬¦ä¸²å¡«å……ä¸ç›¸å…³åˆ—ï¼‰
    average_row = {
        "engine": "å¹³å‡",
        "term": "",
        "status": "",
        "code": "",
        "success": success_rate,
        "content_size_kb": avg_size,
        "elapsed": avg_elapsed,
        "full_url": "",
        "exception": ""
    }
   #æ€»è®¡è¡Œ
    total_row = {
        "engine": "æ€»è®¡",
        "term": "",
        "status": "",
        "code": "",
        "success": f"{success_count}/{total}",
        "content_size_kb": "",
        "elapsed": total_elapsed,
        "full_url": "",
        "exception": ""
    }

    filename = f"csv/{engine}_results_{timestamp}.csv"
    with open(filename, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writerow({k: header_mapping[k] for k in fieldnames})  # è¡¨å¤´
        writer.writerow({k: average_row.get(k, "") for k in fieldnames})  # å¹³å‡
        writer.writerow({k: total_row.get(k, "") for k in fieldnames})  # æ€»è®¡
        for row in data:
            writer.writerow({k: row.get(k, "") for k in fieldnames})  # æ•°æ®

    print(f"âœ… å¼•æ“ {engine} å…± {len(data)} æ¡è®°å½•ä¿å­˜è‡³ {filename}")

# ========= å¯åŠ¨å…¥å£ =========
if __name__ == "__main__":
    asyncio.run(run_all())
