import requests
import concurrent.futures
import csv
import time
import os
from urllib.parse import quote
from requests.exceptions import Timeout, RequestException

# ====================
# **æ ¸å¿ƒé…ç½®åŒº**ï¼ˆå¿…æ”¹éƒ¨åˆ†ï¼‰
# ====================
SEARCH_TERMS = [  # æ˜¾å¼é…ç½®æ£€ç´¢è¯ï¼ˆæ”¯æŒä¸­æ–‡/ç‰¹æ®Šå­—ç¬¦ï¼‰
    "Appleâ€‹",
    "â€‹Breadâ€‹",
    "Cheeseâ€‹",
    "Salmonâ€‹",
    "Chocolate",
    "Spinachâ€‹",
    "Yogurtâ€‹",
    "â€‹Pastaâ€‹",
    "â€‹Almondâ€‹",
    "â€‹Eggplantâ€‹"
]  # 10ä¸ªæ£€ç´¢è¯ï¼ˆç¬¦åˆéœ€æ±‚ï¼‰

# æ–°å¢ï¼šæœåŠ¡å¯ç”¨æ§åˆ¶é…ç½®
SERVICES_ENABLED = {              # å¯ç”¨çš„æœåŠ¡ï¼ˆTrue/Falseï¼‰
    'thordata': False,
    'brightdata': True,
    'oxylabs': False,
    'serpapi': False,
    'yibiaopan': False # æ–°å¢æœåŠ¡å¯ç”¨å¼€å…³
}

SERVICE_CONFIG = {  # è¯·æ›¿æ¢ä¸ºçœŸå®æœåŠ¡å¯†é’¥
    'thordata': {
        'url': "http://170.106.180.18:9004/spiders",
        'headers': {"Content-Type": "application/json"}
    },
    'brightdata': {
        'url': "https://api.brightdata.com/request",
        'headers': {"Authorization": "Bearer 0589abd75f5c73032acb47491ac0693f59ca11ae9dfd6fc36b4390b9ca3109be"},
        'payload': lambda q: {  # å®Œå…¨åŒ¹é…curlå‚æ•°
            "zone": "serp_api1",  # ä¿ç•™ç”¨æˆ·æŒ‡å®šçš„zone
            "url": f"https://www.google.com/search?q={quote(q)}",  # ç›´æ¥ä½¿ç”¨åŸå§‹æ‹¼æ¥ï¼ˆä¸é¢å¤–ç¼–ç ï¼Œä¸curlä¸€è‡´ï¼‰
            "format": "raw"  # åŒ¹é…curlä¸­çš„rawæ ¼å¼
        }
    },
    'oxylabs': {
        'url': "https://realtime.oxylabs.io/v1/queries",
        'auth': ("YOUR_OXYLABS_USER", "YOUR_OXYLABS_PASS"),  # å¿…æ”¹ï¼
        'payload': lambda q: {"source": "google_search", "query": q}
    },
    'yibiaopan': {  # ä¿®æ­£åçš„yibiaopanæœåŠ¡é…ç½®
        'url': "https://scraperapi.thordata.com/request",
        'headers': {
            "Content-Type": "application/json",
            "Authorization": "Bearer 32c62e27a8884eaa7f44252965b46140"  # çœŸå®å¯†é’¥
        },
        'payload': lambda q: {"url": f"https://www.google.com/search?q={q}"}  # ç§»é™¤å¤šä½™çš„&json=1å‚æ•°
    },
    'serpapi': {
        'url': "https://serpapi.abcproxy.com/search",
        'params': lambda q: {"engine": "google", "q": q, "api_key": "YOUR_SERPAPI_KEY"}  # å¿…æ”¹ï¼
    }
}

# åŸºç¡€é…ç½®ï¼ˆå¯è°ƒæ•´ï¼‰
PER_QUERY_REQUESTS = 100    # æ¯ä¸ªè¯è¯·æ±‚æ¬¡æ•°
TIMEOUT_SECONDS = 20        # è¶…æ—¶æ—¶é—´
MIN_FILE_SIZE_KB = 10       # æœ€å°æ–‡ä»¶å¤§å°
CONCURRENCY = 50             # å»ºè®®å¹¶å‘æ•°ï¼ˆé¿å…æœåŠ¡å•†é™æµï¼‰
SAVE_EMPTY_RESPONSE = False  # æ˜¯å¦ä¿å­˜ç©ºå“åº”ï¼ˆè°ƒè¯•æ—¶è®¾ä¸ºTrueï¼‰

# ====================
# å·¥å…·å‡½æ•°ï¼ˆå«å†…å®¹æ ¡éªŒï¼‰
# ====================
def save_response(service, term, req_num, content):
    """å®‰å…¨ä¿å­˜å“åº”å†…å®¹ï¼Œè‡ªåŠ¨å¤„ç†ç©ºå†…å®¹"""
    folder = f"data/{service}"
    os.makedirs(folder, exist_ok=True)
    filename = f"{term[:20].replace('/', '_')}_req{req_num:03d}.txt"  # æˆªæ–­è¿‡é•¿æ£€ç´¢è¯
    path = os.path.join(folder, filename)
    
    if not content and not SAVE_EMPTY_RESPONSE:
        return None  # ä¸ä¿å­˜ç©ºæ–‡ä»¶
    
    with open(path, 'w', encoding='utf-8') as f:
        f.write(content)
    return path

# ====================
# å¢å¼ºå‹è¯·æ±‚å¤„ç†ï¼ˆå«5å±‚æ ¡éªŒï¼‰
# ====================
def handle_request(service, term, req_num):
    """
    5å±‚æ ¡éªŒç¡®ä¿æœ‰æ•ˆå“åº”ï¼š
    1. çŠ¶æ€ç 200
    2. å†…å®¹éç©º
    3. éç©ºç™½å†…å®¹
    4. æ–‡ä»¶å¤§å°è¾¾æ ‡
    5. æœåŠ¡ç‰¹å®šæ ¡éªŒï¼ˆå¦‚JSONæ ¼å¼ï¼‰
    """
    config = SERVICE_CONFIG[service]
    start = time.time()
    try:
        # æ„å»ºè¯·æ±‚
        if service == 'thordata':
            encoded_url = quote(f"https://www.google.com/search?q={term}", safe=':/?=')
            response = requests.get(
                f"{config['url']}?search_url={encoded_url}",
                headers=config['headers'],
                timeout=TIMEOUT_SECONDS
            )
        elif service == 'brightdata':
            response = requests.post(
                config['url'],
                headers=config['headers'],
                json=config['payload'](term),
                timeout=TIMEOUT_SECONDS
            )
        elif service == 'yibiaopan':
            response = requests.post(
                config['url'],
                headers=config['headers'],
                json=config['payload'](term),  # æ­£ç¡®æ„é€ URL
                timeout=TIMEOUT_SECONDS
            )
        elif service == 'oxylabs':
            response = requests.post(
                config['url'],
                auth=config['auth'],
                json=config['payload'](term),
                timeout=TIMEOUT_SECONDS
            )
        elif service == 'serpapi':
            response = requests.get(
                config['url'],
                params=config['params'](term),
                timeout=TIMEOUT_SECONDS
            )
        else:
            raise ValueError("æœªçŸ¥æœåŠ¡")

        elapsed = (time.time() - start) * 1000
        content = response.text.strip()  # å»é™¤é¦–å°¾ç©ºç™½

        # æ ¡éªŒ1ï¼šçŠ¶æ€ç 
        if response.status_code != 200:
            return elapsed, False, f"çŠ¶æ€ç {response.status_code}"
        
        # æ ¡éªŒ2ï¼šå†…å®¹éç©º
        if not content:
            return elapsed, False, "å“åº”å†…å®¹ä¸ºç©º"
        
        # æ ¡éªŒ3ï¼šéç©ºç™½å†…å®¹
        if content.isspace():
            return elapsed, False, "å†…å®¹å…¨ä¸ºç©ºç™½å­—ç¬¦"
        

        # ä¿å­˜æ–‡ä»¶
        file_path = save_response(service, term, req_num, content)
        if not file_path:
            return elapsed, False, "ç©ºå“åº”æœªä¿å­˜"

        # æ ¡éªŒ4ï¼šæ–‡ä»¶å¤§å°
        file_size = os.path.getsize(file_path)
        if file_size < MIN_FILE_SIZE_KB * 1024:
            return elapsed, False, f"æ–‡ä»¶å¤§å°ä¸è¶³({file_size/1024:.2f}KB)"

        return elapsed, True, "æˆåŠŸ"

    except Timeout:
        return TIMEOUT_SECONDS * 1000, False, "è¯·æ±‚è¶…æ—¶"
    except RequestException as e:
        return (time.time() - start) * 1000, False, f"ç½‘ç»œé”™è¯¯: {str(e)}"
    except Exception as e:
        return 0, False, f"è‡´å‘½é”™è¯¯: {str(e)}"

# ====================
# ç»Ÿè®¡ä¸æŠ¥å‘Šï¼ˆç®€åŒ–ç‰ˆï¼Œå»é™¤æ±‡æ€»ç»Ÿè®¡ï¼‰
# ====================
def generate_report(service, stats):
    """ç”Ÿæˆå¸¦è°ƒè¯•ä¿¡æ¯çš„CSVæŠ¥å‘Šï¼ˆä»…è¯¦ç»†è®°å½•ï¼‰"""
    os.makedirs("reports", exist_ok=True)
    filename = f"reports/{service}_report_{time.strftime('%Y%m%d%H%M%S')}.csv"
    
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=[
            'æ—¶é—´æˆ³', 'æœåŠ¡', 'æ£€ç´¢è¯', 'è¯·æ±‚å·', 
            'æ˜¯å¦æˆåŠŸ', 'è€—æ—¶(ms)', 'é”™è¯¯åŸå› ', 'æ–‡ä»¶è·¯å¾„', 'æ–‡ä»¶å¤§å°(KB)'
        ])
        writer.writeheader()

        for item in stats['requests']:
            writer.writerow({
                'æ—¶é—´æˆ³': item['timestamp'],
                'æœåŠ¡': service,
                'æ£€ç´¢è¯': item['term'],
                'è¯·æ±‚å·': item['req_num'],
                'æ˜¯å¦æˆåŠŸ': "âœ”ï¸" if item['success'] else "âŒ",
                'è€—æ—¶(ms)': f"{item['elapsed']:.2f}",
                'é”™è¯¯åŸå› ': item['error'],
                'æ–‡ä»¶è·¯å¾„': item['file_path'] or "-",
                'æ–‡ä»¶å¤§å°(KB)': f"{item['file_size']:.2f}" if item['file_size'] else "-"
            })
    
    return filename

# ====================
# ä¸»æµç¨‹ï¼ˆå¸¦æœåŠ¡å¯ç”¨æ§åˆ¶ï¼‰
# ====================
def main():
    print("=== çˆ¬è™«è¯·æ±‚éªŒè¯å·¥å…· ===")
    print(f"æ£€ç´¢è¯: {len(SEARCH_TERMS)}ä¸ª | æ¯ä¸ªè¯è¯·æ±‚: {PER_QUERY_REQUESTS}æ¬¡")
    print("æ³¨æ„ï¼šè¯·å…ˆæ›¿æ¢SERVICE_CONFIGä¸­çš„çœŸå®å¯†é’¥ï¼\n")

    for service in SERVICE_CONFIG:
        if not SERVICES_ENABLED.get(service, False):  # æ–°å¢æœåŠ¡å¯ç”¨æ£€æŸ¥
            print(f"è·³è¿‡ç¦ç”¨æœåŠ¡: {service}")
            continue

        service_stats = {
            'requests': []  # ç®€åŒ–ç»Ÿè®¡ï¼Œä»…è®°å½•è¯·æ±‚è¯¦æƒ…
        }

        print(f"â–¶ å¼€å§‹å¤„ç†æœåŠ¡: {service}")
        with concurrent.futures.ThreadPoolExecutor(max_workers=CONCURRENCY) as executor:
            futures = {}
            for term_idx, term in enumerate(SEARCH_TERMS, 1):
                for req_idx in range(1, PER_QUERY_REQUESTS+1):
                    future = executor.submit(handle_request, service, term, req_idx)
                    futures[future] = (term, req_idx)

            for future in concurrent.futures.as_completed(futures):
                term, req_idx = futures[future]
                elapsed, success, error = future.result()
                
                timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
                file_path = None
                file_size = 0
                
                if success:
                    file_path = save_response(service, term, req_idx, "")
                    if file_path:
                        file_size = os.path.getsize(file_path) / 1024

                # è®°å½•è¯·æ±‚è¯¦æƒ…ï¼ˆå»é™¤æ±‡æ€»ç»Ÿè®¡ç›¸å…³å­—æ®µï¼‰
                service_stats['requests'].append({
                    'timestamp': timestamp,
                    'term': term,
                    'req_num': req_idx,
                    'success': success,
                    'elapsed': elapsed,
                    'error': error,
                    'file_path': file_path,
                    'file_size': file_size
                })

                # å®æ—¶è¿›åº¦ï¼ˆæ¯50è¯·æ±‚ï¼‰
                if (term_idx * PER_QUERY_REQUESTS + req_idx) % 50 == 0:
                    print(f"è¿›åº¦: {term_idx}/{len(SEARCH_TERMS)} | è¯·æ±‚: {req_idx}/{PER_QUERY_REQUESTS}")

        # ç”ŸæˆæŠ¥å‘Šï¼ˆä»…è¯¦ç»†è®°å½•ï¼Œæ— æ±‡æ€»ç»Ÿè®¡ï¼‰
        report_file = generate_report(service, service_stats)
        print(f"\nğŸ“‹ {service} æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}\n")

if __name__ == "__main__":
    main()