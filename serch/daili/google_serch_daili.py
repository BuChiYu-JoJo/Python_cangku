import requests
import concurrent.futures
import csv
import time
import os
from urllib.parse import quote
import warnings  # æ–°å¢è­¦å‘Šå¤„ç†åº“

# ====================
# **æ ¸å¿ƒé…ç½®åŒº**ï¼ˆå¿…æ”¹éƒ¨åˆ†ï¼‰
# ====================
SEARCH_TERMS = [  
    "Apple", "Bread", "Cheese", "Salmon", "Chocolate",
    "Spinach", "Yogurt", "Pasta", "Almond", "Eggplant"
]

SERVICE_CONFIG = {  
    'thordata': {
        'proxy_user': "td-customer-serp_HhFgxd5euqcb2",  
        'proxy_pass': "JJtvFttulx7n",
        'proxy_host': "scraping.thordata.com",
        'proxy_port': "30001",
    }
}

PER_QUERY_REQUESTS = 1       
TIMEOUT_SECONDS = 20          
CONCURRENCY =   1           
SAVE_EMPTY_RESPONSE = False   

# ====================
# å·¥å…·å‡½æ•°ï¼ˆä¿ç•™æ ¸å¿ƒï¼‰
# ====================
def save_response(term, req_num, content, raw_response=None):  # æ–°å¢raw_responseå‚æ•°
    folder = "data/thordata"
    os.makedirs(folder, exist_ok=True)
    filename = f"{term[:20].replace('/', '_')}_req{req_num:03d}.txt"
    path = os.path.join(folder, filename)

    # ä¿å­˜åŸå§‹å“åº”å†…å®¹ï¼ˆç”¨äºé—®é¢˜å¤ç°ï¼‰
    if raw_response:
        raw_path = os.path.join(folder, f"{os.path.splitext(filename)[0]}_raw.gz")
        with open(raw_path, 'wb') as f:
            f.write(raw_response)

    if not content and not SAVE_EMPTY_RESPONSE:
        return None
    with open(path, 'w', encoding='utf-8') as f:
        f.write(content)
    return path


# ====================
# **thordata ä¸“ç”¨è¯·æ±‚é€»è¾‘**
# ====================
def handle_request(term, req_num):
    config = SERVICE_CONFIG['thordata']
    start = time.time()

    try:
        proxy = f"http://{config['proxy_user']}:{config['proxy_pass']}@{config['proxy_host']}:{config['proxy_port']}"
        proxies = {'http': proxy, 'https': proxy}

        with requests.Session() as session:
            session.proxies = proxies
            session.verify = False  # ä¸šåŠ¡éœ€æ±‚ä¿ç•™
            session.headers.update({  # æ–°å¢ååçˆ¬å¤´éƒ¨
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36',
                'Accept-Encoding': 'gzip, deflate'  # æ˜¾å¼å£°æ˜æ”¯æŒçš„ç¼–ç 
            })
            
            url = f"https://www.google.com/search?q={quote(term, safe=':/?=')}"
            response = session.get(url, timeout=TIMEOUT_SECONDS, stream=True)  # æµå¼è·å–é˜²æ­¢å†…å­˜çˆ†ç‚¸

            # å¼ºåˆ¶ç¦ç”¨è‡ªåŠ¨è§£å‹ï¼ˆå…³é”®ä¿®å¤ï¼‰
            if 'gzip' in response.headers.get('Content-Encoding', ''):
                raw_data = response.content  # ä¿å­˜åŸå§‹gzipæ•°æ®
                try:
                    with gzip.GzipFile(fileobj=BytesIO(raw_data)) as f:
                        content = f.read().decode('utf-8', errors='replace')
                except (gzip.BadGzipFile, OSError) as e:
                    save_response(term, req_num, None, raw_data)  # ä¿å­˜åŸå§‹gzipæ–‡ä»¶ç”¨äºåˆ†æ
                    return (time.time()-start)*1000, False, f"gzipè§£å‹å¤±è´¥: {str(e)}"
            else:
                content = response.text.strip()

            elapsed = (time.time() - start) * 1000

            # å¢å¼ºçš„å†…å®¹æ ¡éªŒ
            if response.status_code != 200:
                return elapsed, False, f"çŠ¶æ€ç {response.status_code}"
            if not content or content.strip() in ['<html><head></head><body></body></html>', '']:
                return elapsed, False, "å†…å®¹æ— æ•ˆï¼ˆç©º/ç©ºç™½é¡µï¼‰"

            file_path = save_response(term, req_num, content, raw_response=raw_data if 'raw_data' in locals() else None)
            if not file_path:
                return elapsed, False, "ç©ºå“åº”æœªä¿å­˜"

            file_size = os.path.getsize(file_path)
            if file_size < 10 * 1024:
                return elapsed, False, f"æ–‡ä»¶è¿‡å°({file_size/1024:.2f}KB)"

            return elapsed, True, "æˆåŠŸ"

    except requests.exceptions.Timeout:
        return TIMEOUT_SECONDS * 1000, False, "è¯·æ±‚è¶…æ—¶"
    except requests.exceptions.RequestException as e:
        return (time.time() - start) * 1000, False, f"ç½‘ç»œé”™è¯¯: {str(e)}"
    except Exception as e:  # æ–°å¢é€šç”¨å¼‚å¸¸æ•è·
        return (time.time() - start) * 1000, False, f"æœªçŸ¥é”™è¯¯: {str(e)}"

# ====================
# ç»Ÿè®¡ä¸æŠ¥å‘Šï¼ˆæç®€ç‰ˆï¼‰
# ====================
def generate_report(stats):
    os.makedirs("reports", exist_ok=True)
    filename = f"reports/thordata_report_{time.strftime('%Y%m%d%H%M%S')}.csv"
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=['æ—¶é—´æˆ³', 'æ£€ç´¢è¯', 'è¯·æ±‚å·', 'æ˜¯å¦æˆåŠŸ', 'è€—æ—¶(ms)', 'é”™è¯¯åŸå› '])
        writer.writeheader()
        for item in stats:
            writer.writerow({
                'æ—¶é—´æˆ³': item['timestamp'],
                'æ£€ç´¢è¯': item['term'],
                'è¯·æ±‚å·': item['req_num'],
                'æ˜¯å¦æˆåŠŸ': "âœ”ï¸" if item['success'] else "âŒ",
                'è€—æ—¶(ms)': f"{item['elapsed']:.2f}",
                'é”™è¯¯åŸå› ': item['error'] or "-"
            })
    return filename

# ====================
# ä¸»æµç¨‹ï¼ˆæ–°å¢è­¦å‘Šå¿½ç•¥é€»è¾‘ï¼‰
# ====================
if __name__ == "__main__":
    # æ¶ˆé™¤InsecureRequestWarningè­¦å‘Šï¼ˆå…³é”®ä¿®æ”¹ï¼‰
    warnings.filterwarnings("ignore", category=requests.packages.urllib3.exceptions.InsecureRequestWarning)
    
    print("=== thordata çˆ¬è™«éªŒè¯å·¥å…· ===")
    print(f"æ£€ç´¢è¯: {len(SEARCH_TERMS)}ä¸ª | æ¯ä¸ªè¯è¯·æ±‚: {PER_QUERY_REQUESTS}æ¬¡\n")
    
    stats = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=CONCURRENCY) as executor:
        futures = {
            executor.submit(handle_request, term, req_num): (term, req_num)
            for term in SEARCH_TERMS
            for req_num in range(1, PER_QUERY_REQUESTS + 1)
        }
        
        for future in concurrent.futures.as_completed(futures):
            term, req_num = futures[future]
            elapsed, success, error = future.result()
            
            stats.append({
                'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
                'term': term,
                'req_num': req_num,
                'success': success,
                'elapsed': elapsed,
                'error': error
            })
            
            total = len(SEARCH_TERMS) * PER_QUERY_REQUESTS
            completed = len(stats)
            print(f"\rè¿›åº¦: {completed}/{total} ({completed/total*100:.1f}%)", end='')
    
    print("\n\nğŸ“‹ æŠ¥å‘Šç”Ÿæˆä¸­...")
    report_file = generate_report(stats)
    print(f"æŠ¥å‘Šå·²ä¿å­˜: {report_file}")