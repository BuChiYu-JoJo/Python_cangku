import http.client
import concurrent.futures
import os
import csv
import logging
import time
import random
from urllib.parse import urlencode
from datetime import datetime

# ====== 配置区域 ======
CONCURRENCY = 5                  # 并发数
TOTAL_REQUESTS = 20              # 总请求次数
SAVE_FILE = True                 # 是否保存返回内容
URLS = [
    {"url": "https://www.google.com", "type": "html"},
    {"url": "https://www.wikipedia.org", "type": "png"},
    {"url": "https://www.bing.com", "type": "html"},
]
API_HOST = "universalapi.thordata.com"
API_PATH = "/request"
AUTH_TOKEN = "Bearer ff3aaeb7605583f7e51675da4ad2a0de"
OUTPUT_DIR = "results"
# =======================

timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
os.makedirs(OUTPUT_DIR, exist_ok=True)
csv_filename = f"{OUTPUT_DIR}/请求结果_并发{CONCURRENCY}_{timestamp}.csv"
log_filename = f"{OUTPUT_DIR}/请求日志_并发{CONCURRENCY}_{timestamp}.log"

# 日志配置
logging.basicConfig(
    filename=log_filename,
    filemode='w',
    format='[%(asctime)s] %(message)s',
    level=logging.INFO,
    encoding='utf-8'
)
logging.info(f"=== 请求并发级别：{CONCURRENCY} | 总请求次数：{TOTAL_REQUESTS} ===")

# 写入 CSV 头
with open(csv_filename, mode='w', newline='', encoding='utf-8') as f:
    writer = csv.writer(f)
    writer.writerow(["请求URL", "响应码", "响应时间（秒）", "返回文件大小（KB）", "错误备注"])

def request_url(args):
    task_id, total = args
    selected = random.choice(URLS)
    url, output_type = selected["url"], selected["type"]

    logging.info(f"=== 开始请求【{task_id}/{total}】===")

    conn = http.client.HTTPSConnection(API_HOST, timeout=30)
    payload = {
        "url": url,
        "type": output_type,
        "js_render": "True"
    }
    form_data = urlencode(payload)
    headers = {
        'Authorization': AUTH_TOKEN,
        'content-type': "application/x-www-form-urlencoded"
    }

    start_time = time.time()
    status_code = "-"
    file_size_kb = "-"
    error_msg = "-"
    try:
        conn.request("POST", API_PATH, form_data, headers)
        res = conn.getresponse()
        data = res.read()
        duration = round(time.time() - start_time, 2)
        status_code = res.status
        file_size_kb = round(len(data) / 1024, 2)

        if SAVE_FILE and status_code == 200:
            file_ext = "html" if output_type == "html" else "png"
            safe_url_part = url.replace('https://', '').replace('http://', '').replace('/', '_')
            filename = f"{task_id}_{safe_url_part}.{file_ext}"
            with open(os.path.join(OUTPUT_DIR, filename), "wb") as f:
                f.write(data)

    except Exception as e:
        duration = round(time.time() - start_time, 2)
        error_msg = str(e)

    # 写入 CSV
    with open(csv_filename, mode='a', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow([url, status_code, duration, file_size_kb, error_msg])

    log_msg = f"任务#{task_id} | URL: {url} | 状态码: {status_code} | 耗时: {duration}s | 大小: {file_size_kb}KB | 错误: {error_msg}"
    logging.info(log_msg)

def main():
    start_time = time.time()
    with concurrent.futures.ThreadPoolExecutor(max_workers=CONCURRENCY) as executor:
        task_args = [(i, TOTAL_REQUESTS) for i in range(1, TOTAL_REQUESTS + 1)]
        executor.map(request_url, task_args)
    total_time = round(time.time() - start_time, 2)
    logging.info(f"=== 所有请求完成，共耗时：{total_time} 秒 ===")

if __name__ == "__main__":
    main()
